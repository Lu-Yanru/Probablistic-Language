// Politeness

// Speaker's utility is presumably more than maximizing informativeness

// state prior, utterance prior, and meaning function
///fold:
var states = [1,2,3,4,5]
var utterances = ["terrible","bad","okay","good","amazing"]

// correspondence of utterances to states (empirically measured)
var literalSemantics = {
  "terrible":[.95,.85,.02,.02,.02],
  "bad":[.85,.95,.02,.02,.02],
  "okay":[0.02,0.25,0.95,.65,.35],
  "good":[.02,.05,.55,.95,.93],
  "amazing":[.02,.02,.02,.65,0.95]
}

// determine whether the utterance describes the state
// by flipping a coin with the literalSemantics weight
// ... state - 1 because of 0-indexing
var meaning = function(utterance, state){
  return flip(literalSemantics[utterance][state - 1]);
};
///

// value function scales social utility by a parameter lambda
var lambda = 1.25 // value taken from MAP estimate from Yoon, Tessler, et al. 2016
var valueFunction = function(s){  // higher the state, higher social value of the state
  return lambda * s
};

// literal listener
var listener0 = function(utterance) {
  Infer({model: function(){
    var state = uniformDraw(states);
    var m = meaning(utterance, state);
    condition(m);
    return state;
  }})
};

var alpha = 10; // MAP estimate from Yoon, Tessler, et al. 2016
var speaker1 = function(state, phi) {
  Infer({model: function(){
    var utterance = uniformDraw(utterances)
    var L0_posterior = listener0(utterance)
    var utility = {
      epistemic: L0_posterior.score(state),
      social: expectation(L0_posterior, valueFunction) // value function of resulting subjective value of literal Listener's state
    }
    var speakerUtility = phi * utility.epistemic + // trade-off btw informativeness and social (but maybe not necessarily dependend)phi: high-> informative, low-> social
                        (1 - phi) * utility.social  // cross-cultural differences possible
    factor(alpha * speakerUtility)
    return utterance
  }})
};

speaker1(1, 0.1)

///
var pragmaticListener = function(utterance) {
  Infer({model: function(){
    var state = uniformDraw(states) // Prior depend on context
    var phi = uniformDraw([0.1, 0.3, 0.5, 0.7, 0.9]) // sample phi
      // if pragmatic listener eg tend to predict that the speaker is social:
      // var phi = categorical()
    var S1 = speaker1(state, phi)
    observe(S1, utterance)
    return { state, phi }
  }})
}

var listenerPosterior = pragmaticListener("good") // the more positive the utterance, pragmatic listener is more likely to assume the speaker to be social
                // reverse for encoding meanness by eg defining utility function as lambda*(-s) -> the more meaness the more utility for the speaker

display("expected state = " +
        expectation(marginalize(listenerPosterior, "state")))
viz(marginalize(listenerPosterior, "state"))

display("expected phi = " +
        expectation(marginalize(listenerPosterior, "phi")))
viz.density(marginalize(listenerPosterior, "phi"))
